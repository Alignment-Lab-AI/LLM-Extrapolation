{
  "results": {
    "hendrycksTest-abstract_algebra": {
      "acc": 0.24,
      "acc_stderr": 0.042923469599092816,
      "acc_norm": 0.24,
      "acc_norm_stderr": 0.042923469599092816
    },
    "hendrycksTest-anatomy": {
      "acc": 0.5333333333333333,
      "acc_stderr": 0.043097329010363554,
      "acc_norm": 0.5333333333333333,
      "acc_norm_stderr": 0.043097329010363554
    },
    "hendrycksTest-astronomy": {
      "acc": 0.6644736842105263,
      "acc_stderr": 0.03842498559395268,
      "acc_norm": 0.6644736842105263,
      "acc_norm_stderr": 0.03842498559395268
    },
    "hendrycksTest-business_ethics": {
      "acc": 0.6,
      "acc_stderr": 0.04923659639173309,
      "acc_norm": 0.6,
      "acc_norm_stderr": 0.04923659639173309
    },
    "hendrycksTest-clinical_knowledge": {
      "acc": 0.5660377358490566,
      "acc_stderr": 0.030503292013342592,
      "acc_norm": 0.5660377358490566,
      "acc_norm_stderr": 0.030503292013342592
    },
    "hendrycksTest-college_biology": {
      "acc": 0.6944444444444444,
      "acc_stderr": 0.03852084696008534,
      "acc_norm": 0.6944444444444444,
      "acc_norm_stderr": 0.03852084696008534
    },
    "hendrycksTest-college_chemistry": {
      "acc": 0.4,
      "acc_stderr": 0.04923659639173309,
      "acc_norm": 0.4,
      "acc_norm_stderr": 0.04923659639173309
    },
    "hendrycksTest-college_computer_science": {
      "acc": 0.54,
      "acc_stderr": 0.05009082659620333,
      "acc_norm": 0.54,
      "acc_norm_stderr": 0.05009082659620333
    },
    "hendrycksTest-college_mathematics": {
      "acc": 0.29,
      "acc_stderr": 0.04560480215720684,
      "acc_norm": 0.29,
      "acc_norm_stderr": 0.04560480215720684
    },
    "hendrycksTest-college_medicine": {
      "acc": 0.5722543352601156,
      "acc_stderr": 0.03772446857518026,
      "acc_norm": 0.5722543352601156,
      "acc_norm_stderr": 0.03772446857518026
    },
    "hendrycksTest-college_physics": {
      "acc": 0.3333333333333333,
      "acc_stderr": 0.04690650298201943,
      "acc_norm": 0.3333333333333333,
      "acc_norm_stderr": 0.04690650298201943
    },
    "hendrycksTest-computer_security": {
      "acc": 0.73,
      "acc_stderr": 0.04461960433384739,
      "acc_norm": 0.73,
      "acc_norm_stderr": 0.04461960433384739
    },
    "hendrycksTest-conceptual_physics": {
      "acc": 0.4851063829787234,
      "acc_stderr": 0.032671518489247764,
      "acc_norm": 0.4851063829787234,
      "acc_norm_stderr": 0.032671518489247764
    },
    "hendrycksTest-econometrics": {
      "acc": 0.43859649122807015,
      "acc_stderr": 0.04668000738510455,
      "acc_norm": 0.43859649122807015,
      "acc_norm_stderr": 0.04668000738510455
    },
    "hendrycksTest-electrical_engineering": {
      "acc": 0.503448275862069,
      "acc_stderr": 0.0416656757710158,
      "acc_norm": 0.503448275862069,
      "acc_norm_stderr": 0.0416656757710158
    },
    "hendrycksTest-elementary_mathematics": {
      "acc": 0.4074074074074074,
      "acc_stderr": 0.02530590624159063,
      "acc_norm": 0.4074074074074074,
      "acc_norm_stderr": 0.02530590624159063
    },
    "hendrycksTest-formal_logic": {
      "acc": 0.49206349206349204,
      "acc_stderr": 0.044715725362943486,
      "acc_norm": 0.49206349206349204,
      "acc_norm_stderr": 0.044715725362943486
    },
    "hendrycksTest-global_facts": {
      "acc": 0.26,
      "acc_stderr": 0.04408440022768079,
      "acc_norm": 0.26,
      "acc_norm_stderr": 0.04408440022768079
    },
    "hendrycksTest-high_school_biology": {
      "acc": 0.7225806451612903,
      "acc_stderr": 0.025470196835900055,
      "acc_norm": 0.7225806451612903,
      "acc_norm_stderr": 0.025470196835900055
    },
    "hendrycksTest-high_school_chemistry": {
      "acc": 0.46798029556650245,
      "acc_stderr": 0.035107665979592154,
      "acc_norm": 0.46798029556650245,
      "acc_norm_stderr": 0.035107665979592154
    },
    "hendrycksTest-high_school_computer_science": {
      "acc": 0.69,
      "acc_stderr": 0.04648231987117316,
      "acc_norm": 0.69,
      "acc_norm_stderr": 0.04648231987117316
    },
    "hendrycksTest-high_school_european_history": {
      "acc": 0.7515151515151515,
      "acc_stderr": 0.03374402644139402,
      "acc_norm": 0.7515151515151515,
      "acc_norm_stderr": 0.03374402644139402
    },
    "hendrycksTest-high_school_geography": {
      "acc": 0.7525252525252525,
      "acc_stderr": 0.030746300742124495,
      "acc_norm": 0.7525252525252525,
      "acc_norm_stderr": 0.030746300742124495
    },
    "hendrycksTest-high_school_government_and_politics": {
      "acc": 0.8186528497409327,
      "acc_stderr": 0.02780703236068609,
      "acc_norm": 0.8186528497409327,
      "acc_norm_stderr": 0.02780703236068609
    },
    "hendrycksTest-high_school_macroeconomics": {
      "acc": 0.5307692307692308,
      "acc_stderr": 0.025302958890850154,
      "acc_norm": 0.5307692307692308,
      "acc_norm_stderr": 0.025302958890850154
    },
    "hendrycksTest-high_school_mathematics": {
      "acc": 0.2740740740740741,
      "acc_stderr": 0.027195934804085626,
      "acc_norm": 0.2740740740740741,
      "acc_norm_stderr": 0.027195934804085626
    },
    "hendrycksTest-high_school_microeconomics": {
      "acc": 0.6176470588235294,
      "acc_stderr": 0.03156663099215416,
      "acc_norm": 0.6176470588235294,
      "acc_norm_stderr": 0.03156663099215416
    },
    "hendrycksTest-high_school_physics": {
      "acc": 0.2582781456953642,
      "acc_stderr": 0.035737053147634576,
      "acc_norm": 0.2582781456953642,
      "acc_norm_stderr": 0.035737053147634576
    },
    "hendrycksTest-high_school_psychology": {
      "acc": 0.7889908256880734,
      "acc_stderr": 0.01749392240411265,
      "acc_norm": 0.7889908256880734,
      "acc_norm_stderr": 0.01749392240411265
    },
    "hendrycksTest-high_school_statistics": {
      "acc": 0.39814814814814814,
      "acc_stderr": 0.033384734032074016,
      "acc_norm": 0.39814814814814814,
      "acc_norm_stderr": 0.033384734032074016
    },
    "hendrycksTest-high_school_us_history": {
      "acc": 0.7696078431372549,
      "acc_stderr": 0.02955429260569506,
      "acc_norm": 0.7696078431372549,
      "acc_norm_stderr": 0.02955429260569506
    },
    "hendrycksTest-high_school_world_history": {
      "acc": 0.7974683544303798,
      "acc_stderr": 0.026160568246601432,
      "acc_norm": 0.7974683544303798,
      "acc_norm_stderr": 0.026160568246601432
    },
    "hendrycksTest-human_aging": {
      "acc": 0.6278026905829597,
      "acc_stderr": 0.032443052830087304,
      "acc_norm": 0.6278026905829597,
      "acc_norm_stderr": 0.032443052830087304
    },
    "hendrycksTest-human_sexuality": {
      "acc": 0.6183206106870229,
      "acc_stderr": 0.04260735157644559,
      "acc_norm": 0.6183206106870229,
      "acc_norm_stderr": 0.04260735157644559
    },
    "hendrycksTest-international_law": {
      "acc": 0.6859504132231405,
      "acc_stderr": 0.04236964753041018,
      "acc_norm": 0.6859504132231405,
      "acc_norm_stderr": 0.04236964753041018
    },
    "hendrycksTest-jurisprudence": {
      "acc": 0.6666666666666666,
      "acc_stderr": 0.04557239513497751,
      "acc_norm": 0.6666666666666666,
      "acc_norm_stderr": 0.04557239513497751
    },
    "hendrycksTest-logical_fallacies": {
      "acc": 0.7423312883435583,
      "acc_stderr": 0.03436150827846917,
      "acc_norm": 0.7423312883435583,
      "acc_norm_stderr": 0.03436150827846917
    },
    "hendrycksTest-machine_learning": {
      "acc": 0.5089285714285714,
      "acc_stderr": 0.04745033255489122,
      "acc_norm": 0.5089285714285714,
      "acc_norm_stderr": 0.04745033255489122
    },
    "hendrycksTest-management": {
      "acc": 0.7184466019417476,
      "acc_stderr": 0.044532548363264673,
      "acc_norm": 0.7184466019417476,
      "acc_norm_stderr": 0.044532548363264673
    },
    "hendrycksTest-marketing": {
      "acc": 0.8247863247863247,
      "acc_stderr": 0.02490443909891824,
      "acc_norm": 0.8247863247863247,
      "acc_norm_stderr": 0.02490443909891824
    },
    "hendrycksTest-medical_genetics": {
      "acc": 0.67,
      "acc_stderr": 0.047258156262526094,
      "acc_norm": 0.67,
      "acc_norm_stderr": 0.047258156262526094
    },
    "hendrycksTest-miscellaneous": {
      "acc": 0.7739463601532567,
      "acc_stderr": 0.014957458504335837,
      "acc_norm": 0.7739463601532567,
      "acc_norm_stderr": 0.014957458504335837
    },
    "hendrycksTest-moral_disputes": {
      "acc": 0.6098265895953757,
      "acc_stderr": 0.026261677607806642,
      "acc_norm": 0.6098265895953757,
      "acc_norm_stderr": 0.026261677607806642
    },
    "hendrycksTest-moral_scenarios": {
      "acc": 0.2849162011173184,
      "acc_stderr": 0.015096222302469799,
      "acc_norm": 0.2849162011173184,
      "acc_norm_stderr": 0.015096222302469799
    },
    "hendrycksTest-nutrition": {
      "acc": 0.6535947712418301,
      "acc_stderr": 0.027245613047215365,
      "acc_norm": 0.6535947712418301,
      "acc_norm_stderr": 0.027245613047215365
    },
    "hendrycksTest-philosophy": {
      "acc": 0.6366559485530546,
      "acc_stderr": 0.027316847674192707,
      "acc_norm": 0.6366559485530546,
      "acc_norm_stderr": 0.027316847674192707
    },
    "hendrycksTest-prehistory": {
      "acc": 0.6512345679012346,
      "acc_stderr": 0.02651759772446501,
      "acc_norm": 0.6512345679012346,
      "acc_norm_stderr": 0.02651759772446501
    },
    "hendrycksTest-professional_accounting": {
      "acc": 0.36879432624113473,
      "acc_stderr": 0.02878222756134725,
      "acc_norm": 0.36879432624113473,
      "acc_norm_stderr": 0.02878222756134725
    },
    "hendrycksTest-professional_law": {
      "acc": 0.41851368970013036,
      "acc_stderr": 0.012599505608336463,
      "acc_norm": 0.41851368970013036,
      "acc_norm_stderr": 0.012599505608336463
    },
    "hendrycksTest-professional_medicine": {
      "acc": 0.5036764705882353,
      "acc_stderr": 0.0303720158854282,
      "acc_norm": 0.5036764705882353,
      "acc_norm_stderr": 0.0303720158854282
    },
    "hendrycksTest-professional_psychology": {
      "acc": 0.619281045751634,
      "acc_stderr": 0.019643801557924806,
      "acc_norm": 0.619281045751634,
      "acc_norm_stderr": 0.019643801557924806
    },
    "hendrycksTest-public_relations": {
      "acc": 0.6272727272727273,
      "acc_stderr": 0.04631381319425465,
      "acc_norm": 0.6272727272727273,
      "acc_norm_stderr": 0.04631381319425465
    },
    "hendrycksTest-security_studies": {
      "acc": 0.5836734693877551,
      "acc_stderr": 0.031557828165561644,
      "acc_norm": 0.5836734693877551,
      "acc_norm_stderr": 0.031557828165561644
    },
    "hendrycksTest-sociology": {
      "acc": 0.7960199004975125,
      "acc_stderr": 0.02849317624532607,
      "acc_norm": 0.7960199004975125,
      "acc_norm_stderr": 0.02849317624532607
    },
    "hendrycksTest-us_foreign_policy": {
      "acc": 0.81,
      "acc_stderr": 0.039427724440366255,
      "acc_norm": 0.81,
      "acc_norm_stderr": 0.039427724440366255
    },
    "hendrycksTest-virology": {
      "acc": 0.41566265060240964,
      "acc_stderr": 0.03836722176598052,
      "acc_norm": 0.41566265060240964,
      "acc_norm_stderr": 0.03836722176598052
    },
    "hendrycksTest-world_religions": {
      "acc": 0.7953216374269005,
      "acc_stderr": 0.030944459778533193,
      "acc_norm": 0.7953216374269005,
      "acc_norm_stderr": 0.030944459778533193
    }
  },
  "versions": {
    "hendrycksTest-abstract_algebra": 1,
    "hendrycksTest-anatomy": 1,
    "hendrycksTest-astronomy": 1,
    "hendrycksTest-business_ethics": 1,
    "hendrycksTest-clinical_knowledge": 1,
    "hendrycksTest-college_biology": 1,
    "hendrycksTest-college_chemistry": 1,
    "hendrycksTest-college_computer_science": 1,
    "hendrycksTest-college_mathematics": 1,
    "hendrycksTest-college_medicine": 1,
    "hendrycksTest-college_physics": 1,
    "hendrycksTest-computer_security": 1,
    "hendrycksTest-conceptual_physics": 1,
    "hendrycksTest-econometrics": 1,
    "hendrycksTest-electrical_engineering": 1,
    "hendrycksTest-elementary_mathematics": 1,
    "hendrycksTest-formal_logic": 1,
    "hendrycksTest-global_facts": 1,
    "hendrycksTest-high_school_biology": 1,
    "hendrycksTest-high_school_chemistry": 1,
    "hendrycksTest-high_school_computer_science": 1,
    "hendrycksTest-high_school_european_history": 1,
    "hendrycksTest-high_school_geography": 1,
    "hendrycksTest-high_school_government_and_politics": 1,
    "hendrycksTest-high_school_macroeconomics": 1,
    "hendrycksTest-high_school_mathematics": 1,
    "hendrycksTest-high_school_microeconomics": 1,
    "hendrycksTest-high_school_physics": 1,
    "hendrycksTest-high_school_psychology": 1,
    "hendrycksTest-high_school_statistics": 1,
    "hendrycksTest-high_school_us_history": 1,
    "hendrycksTest-high_school_world_history": 1,
    "hendrycksTest-human_aging": 1,
    "hendrycksTest-human_sexuality": 1,
    "hendrycksTest-international_law": 1,
    "hendrycksTest-jurisprudence": 1,
    "hendrycksTest-logical_fallacies": 1,
    "hendrycksTest-machine_learning": 1,
    "hendrycksTest-management": 1,
    "hendrycksTest-marketing": 1,
    "hendrycksTest-medical_genetics": 1,
    "hendrycksTest-miscellaneous": 1,
    "hendrycksTest-moral_disputes": 1,
    "hendrycksTest-moral_scenarios": 1,
    "hendrycksTest-nutrition": 1,
    "hendrycksTest-philosophy": 1,
    "hendrycksTest-prehistory": 1,
    "hendrycksTest-professional_accounting": 1,
    "hendrycksTest-professional_law": 1,
    "hendrycksTest-professional_medicine": 1,
    "hendrycksTest-professional_psychology": 1,
    "hendrycksTest-public_relations": 1,
    "hendrycksTest-security_studies": 1,
    "hendrycksTest-sociology": 1,
    "hendrycksTest-us_foreign_policy": 1,
    "hendrycksTest-virology": 1,
    "hendrycksTest-world_religions": 1
  },
  "config": {
    "model": "hf-causal-experimental",
    "model_args": "pretrained=chujiezheng/internlm2-chat-7b-ExPO,use_accelerate=True",
    "num_fewshot": 5,
    "batch_size": "1",
    "batch_sizes": [],
    "device": null,
    "no_cache": false,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {}
  }
}