{
  "results": {
    "hendrycksTest-abstract_algebra": {
      "acc": 0.41,
      "acc_stderr": 0.049431107042371025,
      "acc_norm": 0.41,
      "acc_norm_stderr": 0.049431107042371025
    },
    "hendrycksTest-anatomy": {
      "acc": 0.674074074074074,
      "acc_stderr": 0.040491220417025055,
      "acc_norm": 0.674074074074074,
      "acc_norm_stderr": 0.040491220417025055
    },
    "hendrycksTest-astronomy": {
      "acc": 0.7302631578947368,
      "acc_stderr": 0.03611780560284898,
      "acc_norm": 0.7302631578947368,
      "acc_norm_stderr": 0.03611780560284898
    },
    "hendrycksTest-business_ethics": {
      "acc": 0.64,
      "acc_stderr": 0.04824181513244218,
      "acc_norm": 0.64,
      "acc_norm_stderr": 0.04824181513244218
    },
    "hendrycksTest-clinical_knowledge": {
      "acc": 0.7584905660377359,
      "acc_stderr": 0.026341480371118352,
      "acc_norm": 0.7584905660377359,
      "acc_norm_stderr": 0.026341480371118352
    },
    "hendrycksTest-college_biology": {
      "acc": 0.7638888888888888,
      "acc_stderr": 0.03551446610810826,
      "acc_norm": 0.7638888888888888,
      "acc_norm_stderr": 0.03551446610810826
    },
    "hendrycksTest-college_chemistry": {
      "acc": 0.42,
      "acc_stderr": 0.04960449637488584,
      "acc_norm": 0.42,
      "acc_norm_stderr": 0.04960449637488584
    },
    "hendrycksTest-college_computer_science": {
      "acc": 0.52,
      "acc_stderr": 0.050211673156867795,
      "acc_norm": 0.52,
      "acc_norm_stderr": 0.050211673156867795
    },
    "hendrycksTest-college_mathematics": {
      "acc": 0.33,
      "acc_stderr": 0.047258156262526045,
      "acc_norm": 0.33,
      "acc_norm_stderr": 0.047258156262526045
    },
    "hendrycksTest-college_medicine": {
      "acc": 0.630057803468208,
      "acc_stderr": 0.0368122963339432,
      "acc_norm": 0.630057803468208,
      "acc_norm_stderr": 0.0368122963339432
    },
    "hendrycksTest-college_physics": {
      "acc": 0.5,
      "acc_stderr": 0.04975185951049946,
      "acc_norm": 0.5,
      "acc_norm_stderr": 0.04975185951049946
    },
    "hendrycksTest-computer_security": {
      "acc": 0.77,
      "acc_stderr": 0.042295258468165065,
      "acc_norm": 0.77,
      "acc_norm_stderr": 0.042295258468165065
    },
    "hendrycksTest-conceptual_physics": {
      "acc": 0.5787234042553191,
      "acc_stderr": 0.03227834510146268,
      "acc_norm": 0.5787234042553191,
      "acc_norm_stderr": 0.03227834510146268
    },
    "hendrycksTest-econometrics": {
      "acc": 0.5263157894736842,
      "acc_stderr": 0.046970851366478626,
      "acc_norm": 0.5263157894736842,
      "acc_norm_stderr": 0.046970851366478626
    },
    "hendrycksTest-electrical_engineering": {
      "acc": 0.6551724137931034,
      "acc_stderr": 0.03960933549451208,
      "acc_norm": 0.6551724137931034,
      "acc_norm_stderr": 0.03960933549451208
    },
    "hendrycksTest-elementary_mathematics": {
      "acc": 0.455026455026455,
      "acc_stderr": 0.025646928361049398,
      "acc_norm": 0.455026455026455,
      "acc_norm_stderr": 0.025646928361049398
    },
    "hendrycksTest-formal_logic": {
      "acc": 0.5634920634920635,
      "acc_stderr": 0.04435932892851466,
      "acc_norm": 0.5634920634920635,
      "acc_norm_stderr": 0.04435932892851466
    },
    "hendrycksTest-global_facts": {
      "acc": 0.4,
      "acc_stderr": 0.049236596391733084,
      "acc_norm": 0.4,
      "acc_norm_stderr": 0.049236596391733084
    },
    "hendrycksTest-high_school_biology": {
      "acc": 0.7935483870967742,
      "acc_stderr": 0.023025899617188695,
      "acc_norm": 0.7935483870967742,
      "acc_norm_stderr": 0.023025899617188695
    },
    "hendrycksTest-high_school_chemistry": {
      "acc": 0.5024630541871922,
      "acc_stderr": 0.03517945038691063,
      "acc_norm": 0.5024630541871922,
      "acc_norm_stderr": 0.03517945038691063
    },
    "hendrycksTest-high_school_computer_science": {
      "acc": 0.72,
      "acc_stderr": 0.04512608598542128,
      "acc_norm": 0.72,
      "acc_norm_stderr": 0.04512608598542128
    },
    "hendrycksTest-high_school_european_history": {
      "acc": 0.7515151515151515,
      "acc_stderr": 0.033744026441394036,
      "acc_norm": 0.7515151515151515,
      "acc_norm_stderr": 0.033744026441394036
    },
    "hendrycksTest-high_school_geography": {
      "acc": 0.8131313131313131,
      "acc_stderr": 0.027772533334218957,
      "acc_norm": 0.8131313131313131,
      "acc_norm_stderr": 0.027772533334218957
    },
    "hendrycksTest-high_school_government_and_politics": {
      "acc": 0.8963730569948186,
      "acc_stderr": 0.02199531196364424,
      "acc_norm": 0.8963730569948186,
      "acc_norm_stderr": 0.02199531196364424
    },
    "hendrycksTest-high_school_macroeconomics": {
      "acc": 0.6461538461538462,
      "acc_stderr": 0.024243783994062153,
      "acc_norm": 0.6461538461538462,
      "acc_norm_stderr": 0.024243783994062153
    },
    "hendrycksTest-high_school_mathematics": {
      "acc": 0.4111111111111111,
      "acc_stderr": 0.029999923508706686,
      "acc_norm": 0.4111111111111111,
      "acc_norm_stderr": 0.029999923508706686
    },
    "hendrycksTest-high_school_microeconomics": {
      "acc": 0.7352941176470589,
      "acc_stderr": 0.02865749128507197,
      "acc_norm": 0.7352941176470589,
      "acc_norm_stderr": 0.02865749128507197
    },
    "hendrycksTest-high_school_physics": {
      "acc": 0.3841059602649007,
      "acc_stderr": 0.03971301814719198,
      "acc_norm": 0.3841059602649007,
      "acc_norm_stderr": 0.03971301814719198
    },
    "hendrycksTest-high_school_psychology": {
      "acc": 0.8458715596330275,
      "acc_stderr": 0.015480826865374307,
      "acc_norm": 0.8458715596330275,
      "acc_norm_stderr": 0.015480826865374307
    },
    "hendrycksTest-high_school_statistics": {
      "acc": 0.5370370370370371,
      "acc_stderr": 0.03400603625538272,
      "acc_norm": 0.5370370370370371,
      "acc_norm_stderr": 0.03400603625538272
    },
    "hendrycksTest-high_school_us_history": {
      "acc": 0.8235294117647058,
      "acc_stderr": 0.026756401538078966,
      "acc_norm": 0.8235294117647058,
      "acc_norm_stderr": 0.026756401538078966
    },
    "hendrycksTest-high_school_world_history": {
      "acc": 0.8523206751054853,
      "acc_stderr": 0.02309432958259569,
      "acc_norm": 0.8523206751054853,
      "acc_norm_stderr": 0.02309432958259569
    },
    "hendrycksTest-human_aging": {
      "acc": 0.7309417040358744,
      "acc_stderr": 0.029763779406874965,
      "acc_norm": 0.7309417040358744,
      "acc_norm_stderr": 0.029763779406874965
    },
    "hendrycksTest-human_sexuality": {
      "acc": 0.7633587786259542,
      "acc_stderr": 0.03727673575596913,
      "acc_norm": 0.7633587786259542,
      "acc_norm_stderr": 0.03727673575596913
    },
    "hendrycksTest-international_law": {
      "acc": 0.8016528925619835,
      "acc_stderr": 0.03640118271990946,
      "acc_norm": 0.8016528925619835,
      "acc_norm_stderr": 0.03640118271990946
    },
    "hendrycksTest-jurisprudence": {
      "acc": 0.7685185185185185,
      "acc_stderr": 0.04077494709252626,
      "acc_norm": 0.7685185185185185,
      "acc_norm_stderr": 0.04077494709252626
    },
    "hendrycksTest-logical_fallacies": {
      "acc": 0.754601226993865,
      "acc_stderr": 0.03380939813943354,
      "acc_norm": 0.754601226993865,
      "acc_norm_stderr": 0.03380939813943354
    },
    "hendrycksTest-machine_learning": {
      "acc": 0.5446428571428571,
      "acc_stderr": 0.04726835553719097,
      "acc_norm": 0.5446428571428571,
      "acc_norm_stderr": 0.04726835553719097
    },
    "hendrycksTest-management": {
      "acc": 0.7766990291262136,
      "acc_stderr": 0.04123553189891431,
      "acc_norm": 0.7766990291262136,
      "acc_norm_stderr": 0.04123553189891431
    },
    "hendrycksTest-marketing": {
      "acc": 0.8760683760683761,
      "acc_stderr": 0.02158649400128136,
      "acc_norm": 0.8760683760683761,
      "acc_norm_stderr": 0.02158649400128136
    },
    "hendrycksTest-medical_genetics": {
      "acc": 0.76,
      "acc_stderr": 0.04292346959909283,
      "acc_norm": 0.76,
      "acc_norm_stderr": 0.04292346959909283
    },
    "hendrycksTest-miscellaneous": {
      "acc": 0.8314176245210728,
      "acc_stderr": 0.013387895731543602,
      "acc_norm": 0.8314176245210728,
      "acc_norm_stderr": 0.013387895731543602
    },
    "hendrycksTest-moral_disputes": {
      "acc": 0.7427745664739884,
      "acc_stderr": 0.023532925431044287,
      "acc_norm": 0.7427745664739884,
      "acc_norm_stderr": 0.023532925431044287
    },
    "hendrycksTest-moral_scenarios": {
      "acc": 0.4569832402234637,
      "acc_stderr": 0.01666049858050917,
      "acc_norm": 0.4569832402234637,
      "acc_norm_stderr": 0.01666049858050917
    },
    "hendrycksTest-nutrition": {
      "acc": 0.7352941176470589,
      "acc_stderr": 0.02526169121972948,
      "acc_norm": 0.7352941176470589,
      "acc_norm_stderr": 0.02526169121972948
    },
    "hendrycksTest-philosophy": {
      "acc": 0.729903536977492,
      "acc_stderr": 0.025218040373410616,
      "acc_norm": 0.729903536977492,
      "acc_norm_stderr": 0.025218040373410616
    },
    "hendrycksTest-prehistory": {
      "acc": 0.7222222222222222,
      "acc_stderr": 0.024922001168886335,
      "acc_norm": 0.7222222222222222,
      "acc_norm_stderr": 0.024922001168886335
    },
    "hendrycksTest-professional_accounting": {
      "acc": 0.48936170212765956,
      "acc_stderr": 0.029820747191422473,
      "acc_norm": 0.48936170212765956,
      "acc_norm_stderr": 0.029820747191422473
    },
    "hendrycksTest-professional_law": {
      "acc": 0.4804432855280313,
      "acc_stderr": 0.012760464028289299,
      "acc_norm": 0.4804432855280313,
      "acc_norm_stderr": 0.012760464028289299
    },
    "hendrycksTest-professional_medicine": {
      "acc": 0.6801470588235294,
      "acc_stderr": 0.028332959514031208,
      "acc_norm": 0.6801470588235294,
      "acc_norm_stderr": 0.028332959514031208
    },
    "hendrycksTest-professional_psychology": {
      "acc": 0.704248366013072,
      "acc_stderr": 0.018463154132632813,
      "acc_norm": 0.704248366013072,
      "acc_norm_stderr": 0.018463154132632813
    },
    "hendrycksTest-public_relations": {
      "acc": 0.6636363636363637,
      "acc_stderr": 0.04525393596302506,
      "acc_norm": 0.6636363636363637,
      "acc_norm_stderr": 0.04525393596302506
    },
    "hendrycksTest-security_studies": {
      "acc": 0.7224489795918367,
      "acc_stderr": 0.02866685779027465,
      "acc_norm": 0.7224489795918367,
      "acc_norm_stderr": 0.02866685779027465
    },
    "hendrycksTest-sociology": {
      "acc": 0.8557213930348259,
      "acc_stderr": 0.02484575321230604,
      "acc_norm": 0.8557213930348259,
      "acc_norm_stderr": 0.02484575321230604
    },
    "hendrycksTest-us_foreign_policy": {
      "acc": 0.85,
      "acc_stderr": 0.03588702812826369,
      "acc_norm": 0.85,
      "acc_norm_stderr": 0.03588702812826369
    },
    "hendrycksTest-virology": {
      "acc": 0.5421686746987951,
      "acc_stderr": 0.0387862677100236,
      "acc_norm": 0.5421686746987951,
      "acc_norm_stderr": 0.0387862677100236
    },
    "hendrycksTest-world_religions": {
      "acc": 0.8245614035087719,
      "acc_stderr": 0.029170885500727665,
      "acc_norm": 0.8245614035087719,
      "acc_norm_stderr": 0.029170885500727665
    }
  },
  "versions": {
    "hendrycksTest-abstract_algebra": 1,
    "hendrycksTest-anatomy": 1,
    "hendrycksTest-astronomy": 1,
    "hendrycksTest-business_ethics": 1,
    "hendrycksTest-clinical_knowledge": 1,
    "hendrycksTest-college_biology": 1,
    "hendrycksTest-college_chemistry": 1,
    "hendrycksTest-college_computer_science": 1,
    "hendrycksTest-college_mathematics": 1,
    "hendrycksTest-college_medicine": 1,
    "hendrycksTest-college_physics": 1,
    "hendrycksTest-computer_security": 1,
    "hendrycksTest-conceptual_physics": 1,
    "hendrycksTest-econometrics": 1,
    "hendrycksTest-electrical_engineering": 1,
    "hendrycksTest-elementary_mathematics": 1,
    "hendrycksTest-formal_logic": 1,
    "hendrycksTest-global_facts": 1,
    "hendrycksTest-high_school_biology": 1,
    "hendrycksTest-high_school_chemistry": 1,
    "hendrycksTest-high_school_computer_science": 1,
    "hendrycksTest-high_school_european_history": 1,
    "hendrycksTest-high_school_geography": 1,
    "hendrycksTest-high_school_government_and_politics": 1,
    "hendrycksTest-high_school_macroeconomics": 1,
    "hendrycksTest-high_school_mathematics": 1,
    "hendrycksTest-high_school_microeconomics": 1,
    "hendrycksTest-high_school_physics": 1,
    "hendrycksTest-high_school_psychology": 1,
    "hendrycksTest-high_school_statistics": 1,
    "hendrycksTest-high_school_us_history": 1,
    "hendrycksTest-high_school_world_history": 1,
    "hendrycksTest-human_aging": 1,
    "hendrycksTest-human_sexuality": 1,
    "hendrycksTest-international_law": 1,
    "hendrycksTest-jurisprudence": 1,
    "hendrycksTest-logical_fallacies": 1,
    "hendrycksTest-machine_learning": 1,
    "hendrycksTest-management": 1,
    "hendrycksTest-marketing": 1,
    "hendrycksTest-medical_genetics": 1,
    "hendrycksTest-miscellaneous": 1,
    "hendrycksTest-moral_disputes": 1,
    "hendrycksTest-moral_scenarios": 1,
    "hendrycksTest-nutrition": 1,
    "hendrycksTest-philosophy": 1,
    "hendrycksTest-prehistory": 1,
    "hendrycksTest-professional_accounting": 1,
    "hendrycksTest-professional_law": 1,
    "hendrycksTest-professional_medicine": 1,
    "hendrycksTest-professional_psychology": 1,
    "hendrycksTest-public_relations": 1,
    "hendrycksTest-security_studies": 1,
    "hendrycksTest-sociology": 1,
    "hendrycksTest-us_foreign_policy": 1,
    "hendrycksTest-virology": 1,
    "hendrycksTest-world_religions": 1
  },
  "config": {
    "model": "hf-causal-experimental",
    "model_args": "pretrained=RLHFlow/LLaMA3-iterative-DPO-final,use_accelerate=True",
    "num_fewshot": 5,
    "batch_size": "1",
    "batch_sizes": [],
    "device": null,
    "no_cache": false,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {}
  }
}