{
  "results": {
    "hendrycksTest-abstract_algebra": {
      "acc": 0.35,
      "acc_stderr": 0.04793724854411021,
      "acc_norm": 0.35,
      "acc_norm_stderr": 0.04793724854411021
    },
    "hendrycksTest-anatomy": {
      "acc": 0.5851851851851851,
      "acc_stderr": 0.04256193767901408,
      "acc_norm": 0.5851851851851851,
      "acc_norm_stderr": 0.04256193767901408
    },
    "hendrycksTest-astronomy": {
      "acc": 0.631578947368421,
      "acc_stderr": 0.03925523381052932,
      "acc_norm": 0.631578947368421,
      "acc_norm_stderr": 0.03925523381052932
    },
    "hendrycksTest-business_ethics": {
      "acc": 0.59,
      "acc_stderr": 0.04943110704237102,
      "acc_norm": 0.59,
      "acc_norm_stderr": 0.04943110704237102
    },
    "hendrycksTest-clinical_knowledge": {
      "acc": 0.6716981132075471,
      "acc_stderr": 0.02890159361241178,
      "acc_norm": 0.6716981132075471,
      "acc_norm_stderr": 0.02890159361241178
    },
    "hendrycksTest-college_biology": {
      "acc": 0.6875,
      "acc_stderr": 0.038760854559127644,
      "acc_norm": 0.6875,
      "acc_norm_stderr": 0.038760854559127644
    },
    "hendrycksTest-college_chemistry": {
      "acc": 0.41,
      "acc_stderr": 0.04943110704237102,
      "acc_norm": 0.41,
      "acc_norm_stderr": 0.04943110704237102
    },
    "hendrycksTest-college_computer_science": {
      "acc": 0.48,
      "acc_stderr": 0.05021167315686779,
      "acc_norm": 0.48,
      "acc_norm_stderr": 0.05021167315686779
    },
    "hendrycksTest-college_mathematics": {
      "acc": 0.37,
      "acc_stderr": 0.048523658709391,
      "acc_norm": 0.37,
      "acc_norm_stderr": 0.048523658709391
    },
    "hendrycksTest-college_medicine": {
      "acc": 0.6127167630057804,
      "acc_stderr": 0.03714325906302065,
      "acc_norm": 0.6127167630057804,
      "acc_norm_stderr": 0.03714325906302065
    },
    "hendrycksTest-college_physics": {
      "acc": 0.4117647058823529,
      "acc_stderr": 0.04897104952726366,
      "acc_norm": 0.4117647058823529,
      "acc_norm_stderr": 0.04897104952726366
    },
    "hendrycksTest-computer_security": {
      "acc": 0.69,
      "acc_stderr": 0.04648231987117316,
      "acc_norm": 0.69,
      "acc_norm_stderr": 0.04648231987117316
    },
    "hendrycksTest-conceptual_physics": {
      "acc": 0.5446808510638298,
      "acc_stderr": 0.03255525359340355,
      "acc_norm": 0.5446808510638298,
      "acc_norm_stderr": 0.03255525359340355
    },
    "hendrycksTest-econometrics": {
      "acc": 0.42105263157894735,
      "acc_stderr": 0.046446020912223177,
      "acc_norm": 0.42105263157894735,
      "acc_norm_stderr": 0.046446020912223177
    },
    "hendrycksTest-electrical_engineering": {
      "acc": 0.6,
      "acc_stderr": 0.040824829046386284,
      "acc_norm": 0.6,
      "acc_norm_stderr": 0.040824829046386284
    },
    "hendrycksTest-elementary_mathematics": {
      "acc": 0.3941798941798942,
      "acc_stderr": 0.02516798233389414,
      "acc_norm": 0.3941798941798942,
      "acc_norm_stderr": 0.02516798233389414
    },
    "hendrycksTest-formal_logic": {
      "acc": 0.36507936507936506,
      "acc_stderr": 0.04306241259127153,
      "acc_norm": 0.36507936507936506,
      "acc_norm_stderr": 0.04306241259127153
    },
    "hendrycksTest-global_facts": {
      "acc": 0.36,
      "acc_stderr": 0.04824181513244218,
      "acc_norm": 0.36,
      "acc_norm_stderr": 0.04824181513244218
    },
    "hendrycksTest-high_school_biology": {
      "acc": 0.6838709677419355,
      "acc_stderr": 0.026450874489042767,
      "acc_norm": 0.6838709677419355,
      "acc_norm_stderr": 0.026450874489042767
    },
    "hendrycksTest-high_school_chemistry": {
      "acc": 0.4975369458128079,
      "acc_stderr": 0.03517945038691063,
      "acc_norm": 0.4975369458128079,
      "acc_norm_stderr": 0.03517945038691063
    },
    "hendrycksTest-high_school_computer_science": {
      "acc": 0.65,
      "acc_stderr": 0.047937248544110196,
      "acc_norm": 0.65,
      "acc_norm_stderr": 0.047937248544110196
    },
    "hendrycksTest-high_school_european_history": {
      "acc": 0.7454545454545455,
      "acc_stderr": 0.03401506715249039,
      "acc_norm": 0.7454545454545455,
      "acc_norm_stderr": 0.03401506715249039
    },
    "hendrycksTest-high_school_geography": {
      "acc": 0.7424242424242424,
      "acc_stderr": 0.03115626951964683,
      "acc_norm": 0.7424242424242424,
      "acc_norm_stderr": 0.03115626951964683
    },
    "hendrycksTest-high_school_government_and_politics": {
      "acc": 0.8549222797927462,
      "acc_stderr": 0.02541634309630644,
      "acc_norm": 0.8549222797927462,
      "acc_norm_stderr": 0.02541634309630644
    },
    "hendrycksTest-high_school_macroeconomics": {
      "acc": 0.5717948717948718,
      "acc_stderr": 0.025088301454694827,
      "acc_norm": 0.5717948717948718,
      "acc_norm_stderr": 0.025088301454694827
    },
    "hendrycksTest-high_school_mathematics": {
      "acc": 0.3333333333333333,
      "acc_stderr": 0.028742040903948475,
      "acc_norm": 0.3333333333333333,
      "acc_norm_stderr": 0.028742040903948475
    },
    "hendrycksTest-high_school_microeconomics": {
      "acc": 0.6554621848739496,
      "acc_stderr": 0.030868682604121626,
      "acc_norm": 0.6554621848739496,
      "acc_norm_stderr": 0.030868682604121626
    },
    "hendrycksTest-high_school_physics": {
      "acc": 0.33774834437086093,
      "acc_stderr": 0.0386155754625517,
      "acc_norm": 0.33774834437086093,
      "acc_norm_stderr": 0.0386155754625517
    },
    "hendrycksTest-high_school_psychology": {
      "acc": 0.7908256880733945,
      "acc_stderr": 0.017437937173343233,
      "acc_norm": 0.7908256880733945,
      "acc_norm_stderr": 0.017437937173343233
    },
    "hendrycksTest-high_school_statistics": {
      "acc": 0.4444444444444444,
      "acc_stderr": 0.03388857118502326,
      "acc_norm": 0.4444444444444444,
      "acc_norm_stderr": 0.03388857118502326
    },
    "hendrycksTest-high_school_us_history": {
      "acc": 0.7549019607843137,
      "acc_stderr": 0.03019028245350195,
      "acc_norm": 0.7549019607843137,
      "acc_norm_stderr": 0.03019028245350195
    },
    "hendrycksTest-high_school_world_history": {
      "acc": 0.7552742616033755,
      "acc_stderr": 0.027985699387036423,
      "acc_norm": 0.7552742616033755,
      "acc_norm_stderr": 0.027985699387036423
    },
    "hendrycksTest-human_aging": {
      "acc": 0.6143497757847534,
      "acc_stderr": 0.03266842214289201,
      "acc_norm": 0.6143497757847534,
      "acc_norm_stderr": 0.03266842214289201
    },
    "hendrycksTest-human_sexuality": {
      "acc": 0.7099236641221374,
      "acc_stderr": 0.03980066246467766,
      "acc_norm": 0.7099236641221374,
      "acc_norm_stderr": 0.03980066246467766
    },
    "hendrycksTest-international_law": {
      "acc": 0.8099173553719008,
      "acc_stderr": 0.035817969517092825,
      "acc_norm": 0.8099173553719008,
      "acc_norm_stderr": 0.035817969517092825
    },
    "hendrycksTest-jurisprudence": {
      "acc": 0.7592592592592593,
      "acc_stderr": 0.04133119440243839,
      "acc_norm": 0.7592592592592593,
      "acc_norm_stderr": 0.04133119440243839
    },
    "hendrycksTest-logical_fallacies": {
      "acc": 0.7668711656441718,
      "acc_stderr": 0.0332201579577674,
      "acc_norm": 0.7668711656441718,
      "acc_norm_stderr": 0.0332201579577674
    },
    "hendrycksTest-machine_learning": {
      "acc": 0.38392857142857145,
      "acc_stderr": 0.04616143075028547,
      "acc_norm": 0.38392857142857145,
      "acc_norm_stderr": 0.04616143075028547
    },
    "hendrycksTest-management": {
      "acc": 0.7475728155339806,
      "acc_stderr": 0.04301250399690879,
      "acc_norm": 0.7475728155339806,
      "acc_norm_stderr": 0.04301250399690879
    },
    "hendrycksTest-marketing": {
      "acc": 0.8504273504273504,
      "acc_stderr": 0.02336505149175371,
      "acc_norm": 0.8504273504273504,
      "acc_norm_stderr": 0.02336505149175371
    },
    "hendrycksTest-medical_genetics": {
      "acc": 0.65,
      "acc_stderr": 0.0479372485441102,
      "acc_norm": 0.65,
      "acc_norm_stderr": 0.0479372485441102
    },
    "hendrycksTest-miscellaneous": {
      "acc": 0.7739463601532567,
      "acc_stderr": 0.014957458504335835,
      "acc_norm": 0.7739463601532567,
      "acc_norm_stderr": 0.014957458504335835
    },
    "hendrycksTest-moral_disputes": {
      "acc": 0.684971098265896,
      "acc_stderr": 0.025009313790069727,
      "acc_norm": 0.684971098265896,
      "acc_norm_stderr": 0.025009313790069727
    },
    "hendrycksTest-moral_scenarios": {
      "acc": 0.38994413407821227,
      "acc_stderr": 0.01631237662921307,
      "acc_norm": 0.38994413407821227,
      "acc_norm_stderr": 0.01631237662921307
    },
    "hendrycksTest-nutrition": {
      "acc": 0.6895424836601307,
      "acc_stderr": 0.026493033225145898,
      "acc_norm": 0.6895424836601307,
      "acc_norm_stderr": 0.026493033225145898
    },
    "hendrycksTest-philosophy": {
      "acc": 0.7009646302250804,
      "acc_stderr": 0.026003301117885135,
      "acc_norm": 0.7009646302250804,
      "acc_norm_stderr": 0.026003301117885135
    },
    "hendrycksTest-prehistory": {
      "acc": 0.6882716049382716,
      "acc_stderr": 0.02577311116963045,
      "acc_norm": 0.6882716049382716,
      "acc_norm_stderr": 0.02577311116963045
    },
    "hendrycksTest-professional_accounting": {
      "acc": 0.4432624113475177,
      "acc_stderr": 0.029634838473766002,
      "acc_norm": 0.4432624113475177,
      "acc_norm_stderr": 0.029634838473766002
    },
    "hendrycksTest-professional_law": {
      "acc": 0.43415906127770537,
      "acc_stderr": 0.012659033237067248,
      "acc_norm": 0.43415906127770537,
      "acc_norm_stderr": 0.012659033237067248
    },
    "hendrycksTest-professional_medicine": {
      "acc": 0.6323529411764706,
      "acc_stderr": 0.02928941340940319,
      "acc_norm": 0.6323529411764706,
      "acc_norm_stderr": 0.02928941340940319
    },
    "hendrycksTest-professional_psychology": {
      "acc": 0.6225490196078431,
      "acc_stderr": 0.01961085147488029,
      "acc_norm": 0.6225490196078431,
      "acc_norm_stderr": 0.01961085147488029
    },
    "hendrycksTest-public_relations": {
      "acc": 0.6909090909090909,
      "acc_stderr": 0.044262946482000985,
      "acc_norm": 0.6909090909090909,
      "acc_norm_stderr": 0.044262946482000985
    },
    "hendrycksTest-security_studies": {
      "acc": 0.7061224489795919,
      "acc_stderr": 0.02916273841024977,
      "acc_norm": 0.7061224489795919,
      "acc_norm_stderr": 0.02916273841024977
    },
    "hendrycksTest-sociology": {
      "acc": 0.7263681592039801,
      "acc_stderr": 0.03152439186555402,
      "acc_norm": 0.7263681592039801,
      "acc_norm_stderr": 0.03152439186555402
    },
    "hendrycksTest-us_foreign_policy": {
      "acc": 0.82,
      "acc_stderr": 0.03861229196653694,
      "acc_norm": 0.82,
      "acc_norm_stderr": 0.03861229196653694
    },
    "hendrycksTest-virology": {
      "acc": 0.4879518072289157,
      "acc_stderr": 0.03891364495835821,
      "acc_norm": 0.4879518072289157,
      "acc_norm_stderr": 0.03891364495835821
    },
    "hendrycksTest-world_religions": {
      "acc": 0.8362573099415205,
      "acc_stderr": 0.028380919596145866,
      "acc_norm": 0.8362573099415205,
      "acc_norm_stderr": 0.028380919596145866
    }
  },
  "versions": {
    "hendrycksTest-abstract_algebra": 1,
    "hendrycksTest-anatomy": 1,
    "hendrycksTest-astronomy": 1,
    "hendrycksTest-business_ethics": 1,
    "hendrycksTest-clinical_knowledge": 1,
    "hendrycksTest-college_biology": 1,
    "hendrycksTest-college_chemistry": 1,
    "hendrycksTest-college_computer_science": 1,
    "hendrycksTest-college_mathematics": 1,
    "hendrycksTest-college_medicine": 1,
    "hendrycksTest-college_physics": 1,
    "hendrycksTest-computer_security": 1,
    "hendrycksTest-conceptual_physics": 1,
    "hendrycksTest-econometrics": 1,
    "hendrycksTest-electrical_engineering": 1,
    "hendrycksTest-elementary_mathematics": 1,
    "hendrycksTest-formal_logic": 1,
    "hendrycksTest-global_facts": 1,
    "hendrycksTest-high_school_biology": 1,
    "hendrycksTest-high_school_chemistry": 1,
    "hendrycksTest-high_school_computer_science": 1,
    "hendrycksTest-high_school_european_history": 1,
    "hendrycksTest-high_school_geography": 1,
    "hendrycksTest-high_school_government_and_politics": 1,
    "hendrycksTest-high_school_macroeconomics": 1,
    "hendrycksTest-high_school_mathematics": 1,
    "hendrycksTest-high_school_microeconomics": 1,
    "hendrycksTest-high_school_physics": 1,
    "hendrycksTest-high_school_psychology": 1,
    "hendrycksTest-high_school_statistics": 1,
    "hendrycksTest-high_school_us_history": 1,
    "hendrycksTest-high_school_world_history": 1,
    "hendrycksTest-human_aging": 1,
    "hendrycksTest-human_sexuality": 1,
    "hendrycksTest-international_law": 1,
    "hendrycksTest-jurisprudence": 1,
    "hendrycksTest-logical_fallacies": 1,
    "hendrycksTest-machine_learning": 1,
    "hendrycksTest-management": 1,
    "hendrycksTest-marketing": 1,
    "hendrycksTest-medical_genetics": 1,
    "hendrycksTest-miscellaneous": 1,
    "hendrycksTest-moral_disputes": 1,
    "hendrycksTest-moral_scenarios": 1,
    "hendrycksTest-nutrition": 1,
    "hendrycksTest-philosophy": 1,
    "hendrycksTest-prehistory": 1,
    "hendrycksTest-professional_accounting": 1,
    "hendrycksTest-professional_law": 1,
    "hendrycksTest-professional_medicine": 1,
    "hendrycksTest-professional_psychology": 1,
    "hendrycksTest-public_relations": 1,
    "hendrycksTest-security_studies": 1,
    "hendrycksTest-sociology": 1,
    "hendrycksTest-us_foreign_policy": 1,
    "hendrycksTest-virology": 1,
    "hendrycksTest-world_religions": 1
  },
  "config": {
    "model": "hf-causal-experimental",
    "model_args": "pretrained=snorkelai/Snorkel-Mistral-PairRM-DPO,use_accelerate=True",
    "num_fewshot": 5,
    "batch_size": "1",
    "batch_sizes": [],
    "device": null,
    "no_cache": false,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {}
  }
}