{
  "results": {
    "hendrycksTest-abstract_algebra": {
      "acc": 0.26,
      "acc_stderr": 0.044084400227680794,
      "acc_norm": 0.26,
      "acc_norm_stderr": 0.044084400227680794
    },
    "hendrycksTest-anatomy": {
      "acc": 0.4666666666666667,
      "acc_stderr": 0.043097329010363554,
      "acc_norm": 0.4666666666666667,
      "acc_norm_stderr": 0.043097329010363554
    },
    "hendrycksTest-astronomy": {
      "acc": 0.4868421052631579,
      "acc_stderr": 0.04067533136309173,
      "acc_norm": 0.4868421052631579,
      "acc_norm_stderr": 0.04067533136309173
    },
    "hendrycksTest-business_ethics": {
      "acc": 0.51,
      "acc_stderr": 0.05024183937956912,
      "acc_norm": 0.51,
      "acc_norm_stderr": 0.05024183937956912
    },
    "hendrycksTest-clinical_knowledge": {
      "acc": 0.4981132075471698,
      "acc_stderr": 0.03077265364207567,
      "acc_norm": 0.4981132075471698,
      "acc_norm_stderr": 0.03077265364207567
    },
    "hendrycksTest-college_biology": {
      "acc": 0.4791666666666667,
      "acc_stderr": 0.041775789507399935,
      "acc_norm": 0.4791666666666667,
      "acc_norm_stderr": 0.041775789507399935
    },
    "hendrycksTest-college_chemistry": {
      "acc": 0.34,
      "acc_stderr": 0.04760952285695236,
      "acc_norm": 0.34,
      "acc_norm_stderr": 0.04760952285695236
    },
    "hendrycksTest-college_computer_science": {
      "acc": 0.44,
      "acc_stderr": 0.04988876515698589,
      "acc_norm": 0.44,
      "acc_norm_stderr": 0.04988876515698589
    },
    "hendrycksTest-college_mathematics": {
      "acc": 0.34,
      "acc_stderr": 0.04760952285695235,
      "acc_norm": 0.34,
      "acc_norm_stderr": 0.04760952285695235
    },
    "hendrycksTest-college_medicine": {
      "acc": 0.4508670520231214,
      "acc_stderr": 0.037940126746970296,
      "acc_norm": 0.4508670520231214,
      "acc_norm_stderr": 0.037940126746970296
    },
    "hendrycksTest-college_physics": {
      "acc": 0.27450980392156865,
      "acc_stderr": 0.044405219061793254,
      "acc_norm": 0.27450980392156865,
      "acc_norm_stderr": 0.044405219061793254
    },
    "hendrycksTest-computer_security": {
      "acc": 0.56,
      "acc_stderr": 0.04988876515698589,
      "acc_norm": 0.56,
      "acc_norm_stderr": 0.04988876515698589
    },
    "hendrycksTest-conceptual_physics": {
      "acc": 0.3659574468085106,
      "acc_stderr": 0.0314895582974553,
      "acc_norm": 0.3659574468085106,
      "acc_norm_stderr": 0.0314895582974553
    },
    "hendrycksTest-econometrics": {
      "acc": 0.30701754385964913,
      "acc_stderr": 0.043391383225798615,
      "acc_norm": 0.30701754385964913,
      "acc_norm_stderr": 0.043391383225798615
    },
    "hendrycksTest-electrical_engineering": {
      "acc": 0.41379310344827586,
      "acc_stderr": 0.04104269211806232,
      "acc_norm": 0.41379310344827586,
      "acc_norm_stderr": 0.04104269211806232
    },
    "hendrycksTest-elementary_mathematics": {
      "acc": 0.3148148148148148,
      "acc_stderr": 0.023919984164047732,
      "acc_norm": 0.3148148148148148,
      "acc_norm_stderr": 0.023919984164047732
    },
    "hendrycksTest-formal_logic": {
      "acc": 0.30158730158730157,
      "acc_stderr": 0.04104947269903394,
      "acc_norm": 0.30158730158730157,
      "acc_norm_stderr": 0.04104947269903394
    },
    "hendrycksTest-global_facts": {
      "acc": 0.36,
      "acc_stderr": 0.04824181513244218,
      "acc_norm": 0.36,
      "acc_norm_stderr": 0.04824181513244218
    },
    "hendrycksTest-high_school_biology": {
      "acc": 0.5645161290322581,
      "acc_stderr": 0.02820622559150274,
      "acc_norm": 0.5645161290322581,
      "acc_norm_stderr": 0.02820622559150274
    },
    "hendrycksTest-high_school_chemistry": {
      "acc": 0.4088669950738916,
      "acc_stderr": 0.034590588158832314,
      "acc_norm": 0.4088669950738916,
      "acc_norm_stderr": 0.034590588158832314
    },
    "hendrycksTest-high_school_computer_science": {
      "acc": 0.46,
      "acc_stderr": 0.05009082659620333,
      "acc_norm": 0.46,
      "acc_norm_stderr": 0.05009082659620333
    },
    "hendrycksTest-high_school_european_history": {
      "acc": 0.5636363636363636,
      "acc_stderr": 0.03872592983524754,
      "acc_norm": 0.5636363636363636,
      "acc_norm_stderr": 0.03872592983524754
    },
    "hendrycksTest-high_school_geography": {
      "acc": 0.5555555555555556,
      "acc_stderr": 0.035402943770953675,
      "acc_norm": 0.5555555555555556,
      "acc_norm_stderr": 0.035402943770953675
    },
    "hendrycksTest-high_school_government_and_politics": {
      "acc": 0.6113989637305699,
      "acc_stderr": 0.03517739796373133,
      "acc_norm": 0.6113989637305699,
      "acc_norm_stderr": 0.03517739796373133
    },
    "hendrycksTest-high_school_macroeconomics": {
      "acc": 0.4153846153846154,
      "acc_stderr": 0.02498535492310233,
      "acc_norm": 0.4153846153846154,
      "acc_norm_stderr": 0.02498535492310233
    },
    "hendrycksTest-high_school_mathematics": {
      "acc": 0.2740740740740741,
      "acc_stderr": 0.027195934804085626,
      "acc_norm": 0.2740740740740741,
      "acc_norm_stderr": 0.027195934804085626
    },
    "hendrycksTest-high_school_microeconomics": {
      "acc": 0.4495798319327731,
      "acc_stderr": 0.03231293497137707,
      "acc_norm": 0.4495798319327731,
      "acc_norm_stderr": 0.03231293497137707
    },
    "hendrycksTest-high_school_physics": {
      "acc": 0.2913907284768212,
      "acc_stderr": 0.037101857261199946,
      "acc_norm": 0.2913907284768212,
      "acc_norm_stderr": 0.037101857261199946
    },
    "hendrycksTest-high_school_psychology": {
      "acc": 0.655045871559633,
      "acc_stderr": 0.020380605405066952,
      "acc_norm": 0.655045871559633,
      "acc_norm_stderr": 0.020380605405066952
    },
    "hendrycksTest-high_school_statistics": {
      "acc": 0.3611111111111111,
      "acc_stderr": 0.032757734861009996,
      "acc_norm": 0.3611111111111111,
      "acc_norm_stderr": 0.032757734861009996
    },
    "hendrycksTest-high_school_us_history": {
      "acc": 0.5392156862745098,
      "acc_stderr": 0.03498501649369527,
      "acc_norm": 0.5392156862745098,
      "acc_norm_stderr": 0.03498501649369527
    },
    "hendrycksTest-high_school_world_history": {
      "acc": 0.6540084388185654,
      "acc_stderr": 0.03096481058878671,
      "acc_norm": 0.6540084388185654,
      "acc_norm_stderr": 0.03096481058878671
    },
    "hendrycksTest-human_aging": {
      "acc": 0.484304932735426,
      "acc_stderr": 0.0335412657542081,
      "acc_norm": 0.484304932735426,
      "acc_norm_stderr": 0.0335412657542081
    },
    "hendrycksTest-human_sexuality": {
      "acc": 0.5114503816793893,
      "acc_stderr": 0.043841400240780176,
      "acc_norm": 0.5114503816793893,
      "acc_norm_stderr": 0.043841400240780176
    },
    "hendrycksTest-international_law": {
      "acc": 0.5702479338842975,
      "acc_stderr": 0.04519082021319772,
      "acc_norm": 0.5702479338842975,
      "acc_norm_stderr": 0.04519082021319772
    },
    "hendrycksTest-jurisprudence": {
      "acc": 0.5185185185185185,
      "acc_stderr": 0.04830366024635331,
      "acc_norm": 0.5185185185185185,
      "acc_norm_stderr": 0.04830366024635331
    },
    "hendrycksTest-logical_fallacies": {
      "acc": 0.5276073619631901,
      "acc_stderr": 0.0392237829061099,
      "acc_norm": 0.5276073619631901,
      "acc_norm_stderr": 0.0392237829061099
    },
    "hendrycksTest-machine_learning": {
      "acc": 0.38392857142857145,
      "acc_stderr": 0.04616143075028547,
      "acc_norm": 0.38392857142857145,
      "acc_norm_stderr": 0.04616143075028547
    },
    "hendrycksTest-management": {
      "acc": 0.6699029126213593,
      "acc_stderr": 0.046561471100123514,
      "acc_norm": 0.6699029126213593,
      "acc_norm_stderr": 0.046561471100123514
    },
    "hendrycksTest-marketing": {
      "acc": 0.717948717948718,
      "acc_stderr": 0.029480360549541194,
      "acc_norm": 0.717948717948718,
      "acc_norm_stderr": 0.029480360549541194
    },
    "hendrycksTest-medical_genetics": {
      "acc": 0.56,
      "acc_stderr": 0.04988876515698589,
      "acc_norm": 0.56,
      "acc_norm_stderr": 0.04988876515698589
    },
    "hendrycksTest-miscellaneous": {
      "acc": 0.5836526181353767,
      "acc_stderr": 0.0176279480304303,
      "acc_norm": 0.5836526181353767,
      "acc_norm_stderr": 0.0176279480304303
    },
    "hendrycksTest-moral_disputes": {
      "acc": 0.4797687861271676,
      "acc_stderr": 0.026897049996382868,
      "acc_norm": 0.4797687861271676,
      "acc_norm_stderr": 0.026897049996382868
    },
    "hendrycksTest-moral_scenarios": {
      "acc": 0.24022346368715083,
      "acc_stderr": 0.014288343803925308,
      "acc_norm": 0.24022346368715083,
      "acc_norm_stderr": 0.014288343803925308
    },
    "hendrycksTest-nutrition": {
      "acc": 0.5294117647058824,
      "acc_stderr": 0.028580341065138296,
      "acc_norm": 0.5294117647058824,
      "acc_norm_stderr": 0.028580341065138296
    },
    "hendrycksTest-philosophy": {
      "acc": 0.48231511254019294,
      "acc_stderr": 0.02838032284907713,
      "acc_norm": 0.48231511254019294,
      "acc_norm_stderr": 0.02838032284907713
    },
    "hendrycksTest-prehistory": {
      "acc": 0.5123456790123457,
      "acc_stderr": 0.027812262269327235,
      "acc_norm": 0.5123456790123457,
      "acc_norm_stderr": 0.027812262269327235
    },
    "hendrycksTest-professional_accounting": {
      "acc": 0.30141843971631205,
      "acc_stderr": 0.02737412888263116,
      "acc_norm": 0.30141843971631205,
      "acc_norm_stderr": 0.02737412888263116
    },
    "hendrycksTest-professional_law": {
      "acc": 0.35919165580182527,
      "acc_stderr": 0.012253386187584253,
      "acc_norm": 0.35919165580182527,
      "acc_norm_stderr": 0.012253386187584253
    },
    "hendrycksTest-professional_medicine": {
      "acc": 0.3786764705882353,
      "acc_stderr": 0.029465133639776125,
      "acc_norm": 0.3786764705882353,
      "acc_norm_stderr": 0.029465133639776125
    },
    "hendrycksTest-professional_psychology": {
      "acc": 0.4444444444444444,
      "acc_stderr": 0.020102583895887184,
      "acc_norm": 0.4444444444444444,
      "acc_norm_stderr": 0.020102583895887184
    },
    "hendrycksTest-public_relations": {
      "acc": 0.5636363636363636,
      "acc_stderr": 0.04750185058907297,
      "acc_norm": 0.5636363636363636,
      "acc_norm_stderr": 0.04750185058907297
    },
    "hendrycksTest-security_studies": {
      "acc": 0.47346938775510206,
      "acc_stderr": 0.03196412734523272,
      "acc_norm": 0.47346938775510206,
      "acc_norm_stderr": 0.03196412734523272
    },
    "hendrycksTest-sociology": {
      "acc": 0.6169154228855721,
      "acc_stderr": 0.034375193373382504,
      "acc_norm": 0.6169154228855721,
      "acc_norm_stderr": 0.034375193373382504
    },
    "hendrycksTest-us_foreign_policy": {
      "acc": 0.69,
      "acc_stderr": 0.04648231987117316,
      "acc_norm": 0.69,
      "acc_norm_stderr": 0.04648231987117316
    },
    "hendrycksTest-virology": {
      "acc": 0.41566265060240964,
      "acc_stderr": 0.038367221765980515,
      "acc_norm": 0.41566265060240964,
      "acc_norm_stderr": 0.038367221765980515
    },
    "hendrycksTest-world_religions": {
      "acc": 0.5847953216374269,
      "acc_stderr": 0.037792759455032014,
      "acc_norm": 0.5847953216374269,
      "acc_norm_stderr": 0.037792759455032014
    }
  },
  "versions": {
    "hendrycksTest-abstract_algebra": 1,
    "hendrycksTest-anatomy": 1,
    "hendrycksTest-astronomy": 1,
    "hendrycksTest-business_ethics": 1,
    "hendrycksTest-clinical_knowledge": 1,
    "hendrycksTest-college_biology": 1,
    "hendrycksTest-college_chemistry": 1,
    "hendrycksTest-college_computer_science": 1,
    "hendrycksTest-college_mathematics": 1,
    "hendrycksTest-college_medicine": 1,
    "hendrycksTest-college_physics": 1,
    "hendrycksTest-computer_security": 1,
    "hendrycksTest-conceptual_physics": 1,
    "hendrycksTest-econometrics": 1,
    "hendrycksTest-electrical_engineering": 1,
    "hendrycksTest-elementary_mathematics": 1,
    "hendrycksTest-formal_logic": 1,
    "hendrycksTest-global_facts": 1,
    "hendrycksTest-high_school_biology": 1,
    "hendrycksTest-high_school_chemistry": 1,
    "hendrycksTest-high_school_computer_science": 1,
    "hendrycksTest-high_school_european_history": 1,
    "hendrycksTest-high_school_geography": 1,
    "hendrycksTest-high_school_government_and_politics": 1,
    "hendrycksTest-high_school_macroeconomics": 1,
    "hendrycksTest-high_school_mathematics": 1,
    "hendrycksTest-high_school_microeconomics": 1,
    "hendrycksTest-high_school_physics": 1,
    "hendrycksTest-high_school_psychology": 1,
    "hendrycksTest-high_school_statistics": 1,
    "hendrycksTest-high_school_us_history": 1,
    "hendrycksTest-high_school_world_history": 1,
    "hendrycksTest-human_aging": 1,
    "hendrycksTest-human_sexuality": 1,
    "hendrycksTest-international_law": 1,
    "hendrycksTest-jurisprudence": 1,
    "hendrycksTest-logical_fallacies": 1,
    "hendrycksTest-machine_learning": 1,
    "hendrycksTest-management": 1,
    "hendrycksTest-marketing": 1,
    "hendrycksTest-medical_genetics": 1,
    "hendrycksTest-miscellaneous": 1,
    "hendrycksTest-moral_disputes": 1,
    "hendrycksTest-moral_scenarios": 1,
    "hendrycksTest-nutrition": 1,
    "hendrycksTest-philosophy": 1,
    "hendrycksTest-prehistory": 1,
    "hendrycksTest-professional_accounting": 1,
    "hendrycksTest-professional_law": 1,
    "hendrycksTest-professional_medicine": 1,
    "hendrycksTest-professional_psychology": 1,
    "hendrycksTest-public_relations": 1,
    "hendrycksTest-security_studies": 1,
    "hendrycksTest-sociology": 1,
    "hendrycksTest-us_foreign_policy": 1,
    "hendrycksTest-virology": 1,
    "hendrycksTest-world_religions": 1
  },
  "config": {
    "model": "hf-causal-experimental",
    "model_args": "pretrained=chujiezheng/internlm2-chat-1_8b-ExPO,use_accelerate=True",
    "num_fewshot": 5,
    "batch_size": "1",
    "batch_sizes": [],
    "device": null,
    "no_cache": false,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {}
  }
}