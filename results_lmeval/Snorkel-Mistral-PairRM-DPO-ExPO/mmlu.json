{
  "results": {
    "hendrycksTest-abstract_algebra": {
      "acc": 0.37,
      "acc_stderr": 0.048523658709391,
      "acc_norm": 0.37,
      "acc_norm_stderr": 0.048523658709391
    },
    "hendrycksTest-anatomy": {
      "acc": 0.5925925925925926,
      "acc_stderr": 0.04244633238353227,
      "acc_norm": 0.5925925925925926,
      "acc_norm_stderr": 0.04244633238353227
    },
    "hendrycksTest-astronomy": {
      "acc": 0.6381578947368421,
      "acc_stderr": 0.039105257528497236,
      "acc_norm": 0.6381578947368421,
      "acc_norm_stderr": 0.039105257528497236
    },
    "hendrycksTest-business_ethics": {
      "acc": 0.58,
      "acc_stderr": 0.049604496374885836,
      "acc_norm": 0.58,
      "acc_norm_stderr": 0.049604496374885836
    },
    "hendrycksTest-clinical_knowledge": {
      "acc": 0.6754716981132075,
      "acc_stderr": 0.02881561571343211,
      "acc_norm": 0.6754716981132075,
      "acc_norm_stderr": 0.02881561571343211
    },
    "hendrycksTest-college_biology": {
      "acc": 0.6944444444444444,
      "acc_stderr": 0.03852084696008534,
      "acc_norm": 0.6944444444444444,
      "acc_norm_stderr": 0.03852084696008534
    },
    "hendrycksTest-college_chemistry": {
      "acc": 0.42,
      "acc_stderr": 0.04960449637488584,
      "acc_norm": 0.42,
      "acc_norm_stderr": 0.04960449637488584
    },
    "hendrycksTest-college_computer_science": {
      "acc": 0.47,
      "acc_stderr": 0.05016135580465919,
      "acc_norm": 0.47,
      "acc_norm_stderr": 0.05016135580465919
    },
    "hendrycksTest-college_mathematics": {
      "acc": 0.38,
      "acc_stderr": 0.04878317312145633,
      "acc_norm": 0.38,
      "acc_norm_stderr": 0.04878317312145633
    },
    "hendrycksTest-college_medicine": {
      "acc": 0.5953757225433526,
      "acc_stderr": 0.03742461193887248,
      "acc_norm": 0.5953757225433526,
      "acc_norm_stderr": 0.03742461193887248
    },
    "hendrycksTest-college_physics": {
      "acc": 0.43137254901960786,
      "acc_stderr": 0.04928099597287533,
      "acc_norm": 0.43137254901960786,
      "acc_norm_stderr": 0.04928099597287533
    },
    "hendrycksTest-computer_security": {
      "acc": 0.69,
      "acc_stderr": 0.04648231987117316,
      "acc_norm": 0.69,
      "acc_norm_stderr": 0.04648231987117316
    },
    "hendrycksTest-conceptual_physics": {
      "acc": 0.548936170212766,
      "acc_stderr": 0.032529096196131965,
      "acc_norm": 0.548936170212766,
      "acc_norm_stderr": 0.032529096196131965
    },
    "hendrycksTest-econometrics": {
      "acc": 0.4298245614035088,
      "acc_stderr": 0.04657047260594963,
      "acc_norm": 0.4298245614035088,
      "acc_norm_stderr": 0.04657047260594963
    },
    "hendrycksTest-electrical_engineering": {
      "acc": 0.593103448275862,
      "acc_stderr": 0.04093793981266236,
      "acc_norm": 0.593103448275862,
      "acc_norm_stderr": 0.04093793981266236
    },
    "hendrycksTest-elementary_mathematics": {
      "acc": 0.3835978835978836,
      "acc_stderr": 0.0250437573185202,
      "acc_norm": 0.3835978835978836,
      "acc_norm_stderr": 0.0250437573185202
    },
    "hendrycksTest-formal_logic": {
      "acc": 0.36507936507936506,
      "acc_stderr": 0.04306241259127153,
      "acc_norm": 0.36507936507936506,
      "acc_norm_stderr": 0.04306241259127153
    },
    "hendrycksTest-global_facts": {
      "acc": 0.38,
      "acc_stderr": 0.048783173121456316,
      "acc_norm": 0.38,
      "acc_norm_stderr": 0.048783173121456316
    },
    "hendrycksTest-high_school_biology": {
      "acc": 0.6838709677419355,
      "acc_stderr": 0.026450874489042764,
      "acc_norm": 0.6838709677419355,
      "acc_norm_stderr": 0.026450874489042764
    },
    "hendrycksTest-high_school_chemistry": {
      "acc": 0.49261083743842365,
      "acc_stderr": 0.03517603540361008,
      "acc_norm": 0.49261083743842365,
      "acc_norm_stderr": 0.03517603540361008
    },
    "hendrycksTest-high_school_computer_science": {
      "acc": 0.67,
      "acc_stderr": 0.04725815626252607,
      "acc_norm": 0.67,
      "acc_norm_stderr": 0.04725815626252607
    },
    "hendrycksTest-high_school_european_history": {
      "acc": 0.7393939393939394,
      "acc_stderr": 0.034277431758165236,
      "acc_norm": 0.7393939393939394,
      "acc_norm_stderr": 0.034277431758165236
    },
    "hendrycksTest-high_school_geography": {
      "acc": 0.7424242424242424,
      "acc_stderr": 0.03115626951964683,
      "acc_norm": 0.7424242424242424,
      "acc_norm_stderr": 0.03115626951964683
    },
    "hendrycksTest-high_school_government_and_politics": {
      "acc": 0.8497409326424871,
      "acc_stderr": 0.02578772318072387,
      "acc_norm": 0.8497409326424871,
      "acc_norm_stderr": 0.02578772318072387
    },
    "hendrycksTest-high_school_macroeconomics": {
      "acc": 0.5743589743589743,
      "acc_stderr": 0.025069094387296525,
      "acc_norm": 0.5743589743589743,
      "acc_norm_stderr": 0.025069094387296525
    },
    "hendrycksTest-high_school_mathematics": {
      "acc": 0.34074074074074073,
      "acc_stderr": 0.028897748741131154,
      "acc_norm": 0.34074074074074073,
      "acc_norm_stderr": 0.028897748741131154
    },
    "hendrycksTest-high_school_microeconomics": {
      "acc": 0.6554621848739496,
      "acc_stderr": 0.030868682604121626,
      "acc_norm": 0.6554621848739496,
      "acc_norm_stderr": 0.030868682604121626
    },
    "hendrycksTest-high_school_physics": {
      "acc": 0.33112582781456956,
      "acc_stderr": 0.038425817186598696,
      "acc_norm": 0.33112582781456956,
      "acc_norm_stderr": 0.038425817186598696
    },
    "hendrycksTest-high_school_psychology": {
      "acc": 0.7908256880733945,
      "acc_stderr": 0.017437937173343233,
      "acc_norm": 0.7908256880733945,
      "acc_norm_stderr": 0.017437937173343233
    },
    "hendrycksTest-high_school_statistics": {
      "acc": 0.4583333333333333,
      "acc_stderr": 0.03398110890294636,
      "acc_norm": 0.4583333333333333,
      "acc_norm_stderr": 0.03398110890294636
    },
    "hendrycksTest-high_school_us_history": {
      "acc": 0.7549019607843137,
      "acc_stderr": 0.03019028245350195,
      "acc_norm": 0.7549019607843137,
      "acc_norm_stderr": 0.03019028245350195
    },
    "hendrycksTest-high_school_world_history": {
      "acc": 0.7468354430379747,
      "acc_stderr": 0.028304657943035303,
      "acc_norm": 0.7468354430379747,
      "acc_norm_stderr": 0.028304657943035303
    },
    "hendrycksTest-human_aging": {
      "acc": 0.6233183856502242,
      "acc_stderr": 0.03252113489929189,
      "acc_norm": 0.6233183856502242,
      "acc_norm_stderr": 0.03252113489929189
    },
    "hendrycksTest-human_sexuality": {
      "acc": 0.7099236641221374,
      "acc_stderr": 0.03980066246467766,
      "acc_norm": 0.7099236641221374,
      "acc_norm_stderr": 0.03980066246467766
    },
    "hendrycksTest-international_law": {
      "acc": 0.8016528925619835,
      "acc_stderr": 0.036401182719909476,
      "acc_norm": 0.8016528925619835,
      "acc_norm_stderr": 0.036401182719909476
    },
    "hendrycksTest-jurisprudence": {
      "acc": 0.7592592592592593,
      "acc_stderr": 0.04133119440243839,
      "acc_norm": 0.7592592592592593,
      "acc_norm_stderr": 0.04133119440243839
    },
    "hendrycksTest-logical_fallacies": {
      "acc": 0.7668711656441718,
      "acc_stderr": 0.0332201579577674,
      "acc_norm": 0.7668711656441718,
      "acc_norm_stderr": 0.0332201579577674
    },
    "hendrycksTest-machine_learning": {
      "acc": 0.375,
      "acc_stderr": 0.04595091388086298,
      "acc_norm": 0.375,
      "acc_norm_stderr": 0.04595091388086298
    },
    "hendrycksTest-management": {
      "acc": 0.7572815533980582,
      "acc_stderr": 0.04245022486384495,
      "acc_norm": 0.7572815533980582,
      "acc_norm_stderr": 0.04245022486384495
    },
    "hendrycksTest-marketing": {
      "acc": 0.8504273504273504,
      "acc_stderr": 0.02336505149175371,
      "acc_norm": 0.8504273504273504,
      "acc_norm_stderr": 0.02336505149175371
    },
    "hendrycksTest-medical_genetics": {
      "acc": 0.66,
      "acc_stderr": 0.04760952285695237,
      "acc_norm": 0.66,
      "acc_norm_stderr": 0.04760952285695237
    },
    "hendrycksTest-miscellaneous": {
      "acc": 0.7701149425287356,
      "acc_stderr": 0.015046301846691812,
      "acc_norm": 0.7701149425287356,
      "acc_norm_stderr": 0.015046301846691812
    },
    "hendrycksTest-moral_disputes": {
      "acc": 0.6734104046242775,
      "acc_stderr": 0.02524826477424284,
      "acc_norm": 0.6734104046242775,
      "acc_norm_stderr": 0.02524826477424284
    },
    "hendrycksTest-moral_scenarios": {
      "acc": 0.40893854748603353,
      "acc_stderr": 0.01644283065471554,
      "acc_norm": 0.40893854748603353,
      "acc_norm_stderr": 0.01644283065471554
    },
    "hendrycksTest-nutrition": {
      "acc": 0.6928104575163399,
      "acc_stderr": 0.026415601914388992,
      "acc_norm": 0.6928104575163399,
      "acc_norm_stderr": 0.026415601914388992
    },
    "hendrycksTest-philosophy": {
      "acc": 0.7009646302250804,
      "acc_stderr": 0.026003301117885135,
      "acc_norm": 0.7009646302250804,
      "acc_norm_stderr": 0.026003301117885135
    },
    "hendrycksTest-prehistory": {
      "acc": 0.691358024691358,
      "acc_stderr": 0.025702640260603742,
      "acc_norm": 0.691358024691358,
      "acc_norm_stderr": 0.025702640260603742
    },
    "hendrycksTest-professional_accounting": {
      "acc": 0.450354609929078,
      "acc_stderr": 0.02968010556502904,
      "acc_norm": 0.450354609929078,
      "acc_norm_stderr": 0.02968010556502904
    },
    "hendrycksTest-professional_law": {
      "acc": 0.43546284224250326,
      "acc_stderr": 0.012663412101248333,
      "acc_norm": 0.43546284224250326,
      "acc_norm_stderr": 0.012663412101248333
    },
    "hendrycksTest-professional_medicine": {
      "acc": 0.6433823529411765,
      "acc_stderr": 0.029097209568411952,
      "acc_norm": 0.6433823529411765,
      "acc_norm_stderr": 0.029097209568411952
    },
    "hendrycksTest-professional_psychology": {
      "acc": 0.6274509803921569,
      "acc_stderr": 0.01955964680921593,
      "acc_norm": 0.6274509803921569,
      "acc_norm_stderr": 0.01955964680921593
    },
    "hendrycksTest-public_relations": {
      "acc": 0.6818181818181818,
      "acc_stderr": 0.04461272175910509,
      "acc_norm": 0.6818181818181818,
      "acc_norm_stderr": 0.04461272175910509
    },
    "hendrycksTest-security_studies": {
      "acc": 0.6938775510204082,
      "acc_stderr": 0.029504896454595964,
      "acc_norm": 0.6938775510204082,
      "acc_norm_stderr": 0.029504896454595964
    },
    "hendrycksTest-sociology": {
      "acc": 0.7114427860696517,
      "acc_stderr": 0.03203841040213322,
      "acc_norm": 0.7114427860696517,
      "acc_norm_stderr": 0.03203841040213322
    },
    "hendrycksTest-us_foreign_policy": {
      "acc": 0.82,
      "acc_stderr": 0.03861229196653694,
      "acc_norm": 0.82,
      "acc_norm_stderr": 0.03861229196653694
    },
    "hendrycksTest-virology": {
      "acc": 0.4939759036144578,
      "acc_stderr": 0.03892212195333045,
      "acc_norm": 0.4939759036144578,
      "acc_norm_stderr": 0.03892212195333045
    },
    "hendrycksTest-world_religions": {
      "acc": 0.8304093567251462,
      "acc_stderr": 0.02878210810540171,
      "acc_norm": 0.8304093567251462,
      "acc_norm_stderr": 0.02878210810540171
    }
  },
  "versions": {
    "hendrycksTest-abstract_algebra": 1,
    "hendrycksTest-anatomy": 1,
    "hendrycksTest-astronomy": 1,
    "hendrycksTest-business_ethics": 1,
    "hendrycksTest-clinical_knowledge": 1,
    "hendrycksTest-college_biology": 1,
    "hendrycksTest-college_chemistry": 1,
    "hendrycksTest-college_computer_science": 1,
    "hendrycksTest-college_mathematics": 1,
    "hendrycksTest-college_medicine": 1,
    "hendrycksTest-college_physics": 1,
    "hendrycksTest-computer_security": 1,
    "hendrycksTest-conceptual_physics": 1,
    "hendrycksTest-econometrics": 1,
    "hendrycksTest-electrical_engineering": 1,
    "hendrycksTest-elementary_mathematics": 1,
    "hendrycksTest-formal_logic": 1,
    "hendrycksTest-global_facts": 1,
    "hendrycksTest-high_school_biology": 1,
    "hendrycksTest-high_school_chemistry": 1,
    "hendrycksTest-high_school_computer_science": 1,
    "hendrycksTest-high_school_european_history": 1,
    "hendrycksTest-high_school_geography": 1,
    "hendrycksTest-high_school_government_and_politics": 1,
    "hendrycksTest-high_school_macroeconomics": 1,
    "hendrycksTest-high_school_mathematics": 1,
    "hendrycksTest-high_school_microeconomics": 1,
    "hendrycksTest-high_school_physics": 1,
    "hendrycksTest-high_school_psychology": 1,
    "hendrycksTest-high_school_statistics": 1,
    "hendrycksTest-high_school_us_history": 1,
    "hendrycksTest-high_school_world_history": 1,
    "hendrycksTest-human_aging": 1,
    "hendrycksTest-human_sexuality": 1,
    "hendrycksTest-international_law": 1,
    "hendrycksTest-jurisprudence": 1,
    "hendrycksTest-logical_fallacies": 1,
    "hendrycksTest-machine_learning": 1,
    "hendrycksTest-management": 1,
    "hendrycksTest-marketing": 1,
    "hendrycksTest-medical_genetics": 1,
    "hendrycksTest-miscellaneous": 1,
    "hendrycksTest-moral_disputes": 1,
    "hendrycksTest-moral_scenarios": 1,
    "hendrycksTest-nutrition": 1,
    "hendrycksTest-philosophy": 1,
    "hendrycksTest-prehistory": 1,
    "hendrycksTest-professional_accounting": 1,
    "hendrycksTest-professional_law": 1,
    "hendrycksTest-professional_medicine": 1,
    "hendrycksTest-professional_psychology": 1,
    "hendrycksTest-public_relations": 1,
    "hendrycksTest-security_studies": 1,
    "hendrycksTest-sociology": 1,
    "hendrycksTest-us_foreign_policy": 1,
    "hendrycksTest-virology": 1,
    "hendrycksTest-world_religions": 1
  },
  "config": {
    "model": "hf-causal-experimental",
    "model_args": "pretrained=chujiezheng/Snorkel-Mistral-PairRM-DPO-ExPO,use_accelerate=True",
    "num_fewshot": 5,
    "batch_size": "1",
    "batch_sizes": [],
    "device": null,
    "no_cache": false,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {}
  }
}